{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48534f1c",
   "metadata": {},
   "source": [
    "# LSTM USing Google News Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14da5b47",
   "metadata": {},
   "source": [
    "### Importing Librarties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7937a9b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'sys' (built-in)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from importlib import reload\n",
    "import sys\n",
    "reload(sys)\n",
    "#sys.setdefaultencoding('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c7b1ae",
   "metadata": {},
   "source": [
    "### Predefining Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5123d07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = 'C:/Users/aksha/Downloads/7610 Final/'\n",
    "EMBEDDING_FILE = BASE_DIR + 'GoogleNews-vectors-negative300.bin'\n",
    "TRAIN_DATA_FILE = BASE_DIR + 'train.csv'\n",
    "TEST_DATA_FILE = BASE_DIR + 'test.csv'\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "num_lstm = np.random.randint(175, 275)\n",
    "num_dense = np.random.randint(100, 150)\n",
    "rate_drop_lstm = 0.15 + np.random.rand() * 0.25\n",
    "rate_drop_dense = 0.15 + np.random.rand() * 0.25\n",
    "\n",
    "act = 'relu'\n",
    "re_weight = True # whether to re-weight classes to fit the 17.5% share in test set\n",
    "\n",
    "STAMP = 'lstm_%d_%d_%.2f_%.2f'%(num_lstm, num_dense, rate_drop_lstm, \\\n",
    "        rate_drop_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38931792",
   "metadata": {},
   "source": [
    "### Indexing Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72642443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n",
      "Found 3000000 word vectors of word2vec\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors')\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, \\\n",
    "        binary=True)\n",
    "print('Found %s word vectors of word2vec' % len(word2vec.key_to_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e004040f",
   "metadata": {},
   "source": [
    "### Preprocessing, Removing Stopwords, Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bde3e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad7afe9",
   "metadata": {},
   "source": [
    "### Tokenization and Padding Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "942a0c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 404290 texts in train.csv\n",
      "Found 2345796 texts in test.csv\n",
      "Found 120499 unique tokens\n",
      "Shape of data tensor: (404290, 30)\n",
      "Shape of label tensor: (404290,)\n"
     ]
    }
   ],
   "source": [
    "texts_1 = [] \n",
    "texts_2 = []\n",
    "labels = []\n",
    "with codecs.open(TRAIN_DATA_FILE, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    for values in reader:\n",
    "        texts_1.append(text_to_wordlist(values[3]))\n",
    "        texts_2.append(text_to_wordlist(values[4]))\n",
    "        labels.append(int(values[5]))\n",
    "print('Found %s texts in train.csv' % len(texts_1))\n",
    "\n",
    "test_texts_1 = []\n",
    "test_texts_2 = []\n",
    "test_ids = []\n",
    "with codecs.open(TEST_DATA_FILE, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    for values in reader:\n",
    "        test_texts_1.append(text_to_wordlist(values[1]))\n",
    "        test_texts_2.append(text_to_wordlist(values[2]))\n",
    "        test_ids.append(values[0])\n",
    "print('Found %s texts in test.csv' % len(test_texts_1))\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts_1 + texts_2 + test_texts_1 + test_texts_2)\n",
    "\n",
    "sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
    "sequences_2 = tokenizer.texts_to_sequences(texts_2)\n",
    "test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n",
    "test_sequences_2 = tokenizer.texts_to_sequences(test_texts_2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "data_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_2 = pad_sequences(sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(labels)\n",
    "print('Shape of data tensor:', data_1.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_ids = np.array(test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f54db6",
   "metadata": {},
   "source": [
    "### Embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3172070b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aksha\\AppData\\Local\\Temp/ipykernel_22088/1628195036.py:8: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
      "  embedding_matrix[i] = word2vec.word_vec(word)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 61789\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix')\n",
    "\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.key_to_index:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78414772",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = np.random.permutation(len(data_1))\n",
    "idx_train = perm[:int(len(data_1)*(1-VALIDATION_SPLIT))]\n",
    "idx_val = perm[int(len(data_1)*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "data_1_train = np.vstack((data_1[idx_train], data_2[idx_train]))\n",
    "data_2_train = np.vstack((data_2[idx_train], data_1[idx_train]))\n",
    "labels_train = np.concatenate((labels[idx_train], labels[idx_train]))\n",
    "\n",
    "data_1_val = np.vstack((data_1[idx_val], data_2[idx_val]))\n",
    "data_2_val = np.vstack((data_2[idx_val], data_1[idx_val]))\n",
    "labels_val = np.concatenate((labels[idx_val], labels[idx_val]))\n",
    "\n",
    "weight_val = np.ones(len(labels_val))\n",
    "if re_weight:\n",
    "    weight_val *= 0.472001959\n",
    "    weight_val[labels_val==0] = 1.309028344"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337426b0",
   "metadata": {},
   "source": [
    "### Modeling LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "122393c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(nb_words,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False)\n",
    "lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "\n",
    "sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "merged = concatenate([x1, y1])\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "merged = Dense(num_dense, activation=act)(merged)\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "preds = Dense(1, activation='sigmoid')(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31ba7e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "if re_weight:\n",
    "    class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "else:\n",
    "    class_weight = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b7fba40",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm_263_123_0.27_0.35\n",
      "Epoch 1/27\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.3785 - acc: 0.6906WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 1197s 3s/step - loss: 0.3785 - acc: 0.6906 - val_loss: 0.3562 - val_acc: 0.6924\n",
      "Epoch 2/27\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.3509 - acc: 0.7135WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 934s 3s/step - loss: 0.3509 - acc: 0.7135 - val_loss: 0.3302 - val_acc: 0.7172\n",
      "Epoch 3/27\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.3351 - acc: 0.7276WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 926s 3s/step - loss: 0.3351 - acc: 0.7276 - val_loss: 0.3187 - val_acc: 0.7244\n",
      "Epoch 4/27\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.3234 - acc: 0.7390WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 902s 3s/step - loss: 0.3234 - acc: 0.7390 - val_loss: 0.3137 - val_acc: 0.7598\n",
      "Epoch 5/27\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.3143 - acc: 0.7472WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 910s 3s/step - loss: 0.3143 - acc: 0.7472 - val_loss: 0.3031 - val_acc: 0.7618\n",
      "Epoch 6/27\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.3063 - acc: 0.7553WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 888s 2s/step - loss: 0.3063 - acc: 0.7553 - val_loss: 0.3037 - val_acc: 0.7728\n",
      "Epoch 7/27\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.2994 - acc: 0.7619WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 893s 3s/step - loss: 0.2994 - acc: 0.7619 - val_loss: 0.2942 - val_acc: 0.7744\n",
      "Epoch 8/27\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.2933 - acc: 0.7684WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 892s 3s/step - loss: 0.2933 - acc: 0.7684 - val_loss: 0.2956 - val_acc: 0.7915\n",
      "Epoch 9/27\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.2878 - acc: 0.7738WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 880s 2s/step - loss: 0.2878 - acc: 0.7738 - val_loss: 0.2865 - val_acc: 0.7833\n",
      "Epoch 10/27\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.2820 - acc: 0.7792WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 884s 2s/step - loss: 0.2820 - acc: 0.7792 - val_loss: 0.2837 - val_acc: 0.7926\n",
      "Epoch 11/27\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.2773 - acc: 0.7842WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 888s 2s/step - loss: 0.2773 - acc: 0.7842 - val_loss: 0.2791 - val_acc: 0.7856\n",
      "Epoch 12/27\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.2732 - acc: 0.7879WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 879s 2s/step - loss: 0.2732 - acc: 0.7879 - val_loss: 0.2819 - val_acc: 0.7963\n",
      "Epoch 13/27\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.2687 - acc: 0.7917WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 885s 2s/step - loss: 0.2687 - acc: 0.7917 - val_loss: 0.2770 - val_acc: 0.7994\n",
      "Epoch 14/27\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.2648 - acc: 0.7962WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 866s 2s/step - loss: 0.2648 - acc: 0.7962 - val_loss: 0.2711 - val_acc: 0.7896\n",
      "Epoch 15/27\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.2611 - acc: 0.7996WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 884s 2s/step - loss: 0.2611 - acc: 0.7996 - val_loss: 0.2746 - val_acc: 0.8026\n",
      "Epoch 16/27\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.2569 - acc: 0.8039WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 874s 2s/step - loss: 0.2569 - acc: 0.8039 - val_loss: 0.2731 - val_acc: 0.8028\n",
      "Epoch 17/27\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.2535 - acc: 0.8068WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "356/356 [==============================] - 889s 2s/step - loss: 0.2535 - acc: 0.8068 - val_loss: 0.2667 - val_acc: 0.8008\n",
      "Epoch 18/27\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.2500 - acc: 0.8099WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 884s 2s/step - loss: 0.2500 - acc: 0.8099 - val_loss: 0.2796 - val_acc: 0.8112\n",
      "Epoch 19/27\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.2480 - acc: 0.8120WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 886s 2s/step - loss: 0.2480 - acc: 0.8120 - val_loss: 0.2705 - val_acc: 0.8041\n",
      "Epoch 20/27\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.2447 - acc: 0.8153WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 889s 2s/step - loss: 0.2447 - acc: 0.8153 - val_loss: 0.2654 - val_acc: 0.8055\n",
      "Epoch 21/27\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.2421 - acc: 0.8177WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 890s 2s/step - loss: 0.2421 - acc: 0.8177 - val_loss: 0.2661 - val_acc: 0.8105\n",
      "Epoch 22/27\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.2394 - acc: 0.8203WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 883s 2s/step - loss: 0.2394 - acc: 0.8203 - val_loss: 0.2675 - val_acc: 0.8099\n",
      "Epoch 23/27\n",
      "356/356 [==============================] - ETA: 0s - loss: 0.2372 - acc: 0.8220WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "356/356 [==============================] - 880s 2s/step - loss: 0.2372 - acc: 0.8220 - val_loss: 0.2714 - val_acc: 0.8161\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[sequence_1_input, sequence_2_input], \\\n",
    "        outputs=preds)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "        optimizer='nadam',\n",
    "        metrics=['acc'])\n",
    "#model.summary()\n",
    "print(STAMP)\n",
    "\n",
    "early_stopping =EarlyStopping(monitor='val_loss', patience=3)\n",
    "bst_model_path = STAMP + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "hist = model.fit([data_1_train, data_2_train], labels_train, \\\n",
    "        validation_data=([data_1_val, data_2_val], labels_val, weight_val), \\\n",
    "        epochs=27, batch_size=2048, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "model.load_weights(bst_model_path)\n",
    "bst_val_score = min(hist.history['val_loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d026fa58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287/287 [==============================] - 516s 2s/step\n",
      "287/287 [==============================] - 521s 2s/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict([test_data_1, test_data_2], batch_size=8192, verbose=1)\n",
    "preds += model.predict([test_data_2, test_data_1], batch_size=8192, verbose=1)\n",
    "preds /= 2\n",
    "\n",
    "results = pd.DataFrame({'test_id':test_ids, 'is_duplicate':preds.ravel()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "830405b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.106776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.403705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.002177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.515709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  test_id  is_duplicate\n",
       "0       0      0.001727\n",
       "1       1      0.106776\n",
       "2       2      0.403705\n",
       "3       3      0.002177\n",
       "4       4      0.515709"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d59ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
